# -*- coding: utf-8 -*-
"""Final Project ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YrFfcq_h9z6we2BHF1zRurX9-p9tY6G
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow.keras import models, layers
from google.colab import drive
from scipy.ndimage.filters import uniform_filter1d

# mount drive to upload and access data
drive.mount('/content/drive')

# get data from files
train = np.loadtxt("../exoTrain.csv", delimiter=",", skiprows=1)
test = np.loadtxt("../exoTest.csv", delimiter=",", skiprows=1)

print(train)                              # print train to view data for extracting train and test matrices 

x_train = train[:, 1:]                    # select all rows and every column but the label column
y_train = train[:, 0, np.newaxis] - 1     # select all rows and the label column, subtract one for labels to be 0/1 rather than 1/2

x_test = test[:, 1:]                      # select all rows and every column but the label column
y_test = test[:, 0, np.newaxis] - 1       # select all rows and the label column, subtract one for labels to be 0/1 rather than 1/2

x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1,1)) / np.std(x_train, axis=1).reshape(-1,1))    # normalize
x_test = ((x_test - np.mean(x_test, axis=1).reshape(-1,1)) / np.std(x_test, axis=1).reshape(-1,1))        # normalize

x_train = np.stack([x_train], axis=2)     # alters data on new axis to use in model
x_test = np.stack([x_test], axis=2)       # alters data on new axis to use in model

print(x_train.shape[1:])                  # print x_train shape to verify np.stack worked

# build model
model = models.Sequential()               
model.add(layers.Conv1D(filters=4, kernel_size=3, strides=2, activation="relu", input_shape=(x_train.shape[1:])))
model.add(layers.MaxPooling1D(pool_size=2))
model.add(layers.BatchNormalization())
model.add(layers.Conv1D(filters=16, kernel_size=6, strides=2, activation="relu"))
model.add(layers.MaxPooling1D(pool_size=2))
model.add(layers.Conv1D(filters=64, kernel_size=9, strides=2, activation="relu"))
model.add(layers.BatchNormalization())
model.add(layers.Flatten())
model.add(layers.Dense(128, activation="relu"))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(48, activation="relu"))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(24, activation="relu"))
model.add(layers.Dense(1, activation="relu"))

model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])    # compile model
model.summary()                                                                                     # show model summary with parameters 
results = model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), verbose=2, epochs=20)   # fit model using the training data
model.evaluate(x=x_test, y=y_test, verbose=2)                                                       # evaluate model using the testing data

plt.plot(results.history['loss'], color='b')
plt.plot(results.history['val_loss'], color='r')
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.show()
plt.plot(results.history['accuracy'], color='b')
plt.plot(results.history['val_accuracy'], color='r')
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.show()